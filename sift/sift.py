import cv2
import numpy as np
import pickle
import json


speedSigns = [  "20",
				"30",
				"40",
				"50",
				"60",
				"70",
				"80",
				"100",
				"120"]

imgKps = []
imgDescs = []

with open('sift/siftDescriptors.pkl', 'rb') as inputs:
	for sign in speedSigns:
		rawKeys = pickle.load(inputs)
		keypoints = []
		for kp in rawKeys:
			p = cv2.KeyPoint(x=kp["point0"],y=kp["point0"],size=kp["size"], angle=kp["angle"], response=kp["response"], octave=kp["octave"])
			keypoints.append(p)
		imgKps.append(keypoints)

		desc = pickle.load(inputs)
		imgDescs.append(desc)



def avgKeypointPosition(matches, imgKp):
	if len(matches) == 0:
		return None
	else:
		points = np.zeros((len(matches), 2))
		for i, match in enumerate(matches):
			(x,y) = imgKp[match[0].trainIdx].pt
			points[i, 0] = x
			points[i, 1] = y
		mean_point = np.mean(points, axis=0)
		return tuple([int(x) for x in mean_point])

def detect(image):
	MIN_MATCH_COUNT = 10
	# Initialize SIFT detector
	sift = cv2.SIFT_create(contrastThreshold=0.1)#contrastThreshold=0.1

	#comparisonImage = cv2.imread("gimp/60_2.jpg")
	#comparisonImage = cv2.resize(comparisonImage, dsize=(160, 160), interpolation=cv2.INTER_CUBIC) #256

	gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #
	#compGray = cv2.cvtColor(comparisonImage, cv2.COLOR_BGR2GRAY) #

	#gray = cv2.GaussianBlur(gray, (7,7), 0 )
	#compGray = cv2.GaussianBlur(compGray, (7,7), 0 )
	#gray = cv2.resize(gray, (gray.shape[0]*2, gray.shape[1]*2), interpolation = cv2.INTER_AREA)
	"""
	kp = sift.detect(compGray,None)
	#comparisonImage = cv2.drawKeypoints(compGray,kp,comparisonImage)
	kp = sift.detect(comparisonImage,None)
	image = cv2.drawKeypoints(gray,kp,image)
	"""
	#signKp, signDesc = sift.detectAndCompute(compGray, None)
	imgKp, imgDesc = sift.detectAndCompute(gray, None)
	

	FLANN_INDEX_KDTREE = 0
	index = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
	search = dict(checks=50)#50
	matcher = cv2.FlannBasedMatcher(index, search)

	foundBySign = []

	for i in range(len(imgKps)):
		found = []
		signKp = imgKps[i]
		signDesc = imgDescs[i]
		matches = matcher.knnMatch(signDesc, imgDesc, k=2)
		
		for match in matches:
			if match[0].distance< 0.5 * match[1].distance: #0.25 #original 0.1
				found.append(match)
			#print(match[0].distance,  match[1].distance)
			#print(match[0].trainIdx)
		foundBySign.append(found)

	knn_image = image

	#generated by chatgpt, it's pretty neat
	signWithMostFound = max(range(len(foundBySign)), key=lambda i: len(foundBySign[i]))

	mostFittingFound = foundBySign[signWithMostFound]
	mostFittingSignLabel = speedSigns[signWithMostFound]

	if (len(mostFittingFound) != 0):
		(x, y) = avgKeypointPosition(mostFittingFound, imgKp)
		cv2.circle(image, (x, y), 25, (255, 0, 255), 2)


		cv2.putText(image, mostFittingSignLabel, [x,y-40], cv2.FONT_HERSHEY_SIMPLEX, 
					1, (255,0,255), 2, cv2.LINE_AA)


	"""
	for match in found:
		(x,y) = imgKp[match[0].trainIdx].pt
		x = int(x)
		y = int(y)
		cv2.circle(image, (x, y), 25, (0, 255, 0), 2)
	"""

	#if (len(found) != 0):
		#knn_image = cv2.drawMatchesKnn(comparisonImage, signKp, image, imgKp, matches, None, flags=2)
	return image